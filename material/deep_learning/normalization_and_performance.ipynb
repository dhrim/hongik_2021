{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "normalization_and_performance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH--ADDvhXQM"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfBfbJqOyIkL"
      },
      "source": [
        "# Normalization을 하지 않으면\n",
        "\n",
        "위에서는 0 ~ 255의 값을 0 ~ 1로 normalization하여 학습했다.\n",
        "\n",
        "이 과정을 생략하고 그대로 실행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0BQDjm2yYpH"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# train_x = train_x/255\n",
        "# test_x = test_x/255\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlMkT8axIoi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "1f0ff1a3-3b47-4cee-fc5d-f10f0b46b573"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(train_x, train_y, epochs=5, verbose=1, batch_size=128)\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss=\",loss)\n",
        "print(\"acc=\",acc)\n",
        "\n",
        "y_ = model.predict(test_x)\n",
        "predicted = np.argmax(y_, axis=1)\n",
        "\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 8,070\n",
            "Trainable params: 8,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 2.4654 - acc: 0.1538\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 1.9504 - acc: 0.2509\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 1.7553 - acc: 0.3037\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.6404 - acc: 0.3354\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.5597 - acc: 0.3471\n",
            "10000/10000 [==============================] - 0s 26us/sample - loss: 1.5352 - acc: 0.3417\n",
            "loss= 1.5351685056686402\n",
            "acc= 0.3417\n",
            "[9 6 1 ... 9 2 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_H2aV8qy0Im"
      },
      "source": [
        "학습이 진행되지만 normalization했을 때와 비교하면 더디게 진행된다.\n",
        "\n",
        "아래는 0~1로 normalization을 했을 때\n",
        "```\n",
        "Epoch 1/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.8862 - acc: 0.7278\n",
        "Epoch 2/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3659 - acc: 0.8950\n",
        "Epoch 3/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3164 - acc: 0.9100\n",
        "Epoch 4/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2904 - acc: 0.9171\n",
        "Epoch 5/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2724 - acc: 0.9225\n",
        "10000/10000 [==============================] - 0s 27us/sample - loss: 0.2613 - acc: 0.9265\n",
        "loss= 0.26132496373653413\n",
        "acc= 0.9265\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki5TbWWT-DY7"
      },
      "source": [
        "# -1 ~ 1로 Normalization 하면"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlldGHxzygP8"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# train_x = train_x/255\n",
        "# test_x = test_x/255\n",
        "train_x = train_x/127.5 - 1\n",
        "test_x = test_x/127.5 - 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Evje8DO-MmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "a8663ac9-8669-4948-d71b-8d23c6855fdd"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(train_x, train_y, epochs=5, verbose=1, batch_size=128)\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss=\",loss)\n",
        "print(\"acc=\",acc)\n",
        "\n",
        "y_ = model.predict(test_x)\n",
        "predicted = np.argmax(y_, axis=1)\n",
        "\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 8,070\n",
            "Trainable params: 8,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.3357 - acc: 0.5336\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.7027 - acc: 0.7702\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.6038 - acc: 0.8077\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.5583 - acc: 0.8249\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.5221 - acc: 0.8389\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 0.4942 - acc: 0.8463\n",
            "loss= 0.49422486678361893\n",
            "acc= 0.8463\n",
            "[7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kacaLSWw_DSE"
      },
      "source": [
        "학습이 진행되지만 normalization했을 때와 비교하면 더디게 진행된다.\n",
        "\n",
        "아래는 0~1로 normalization을 했을 때\n",
        "```\n",
        "Epoch 1/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.8862 - acc: 0.7278\n",
        "Epoch 2/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3659 - acc: 0.8950\n",
        "Epoch 3/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3164 - acc: 0.9100\n",
        "Epoch 4/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2904 - acc: 0.9171\n",
        "Epoch 5/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2724 - acc: 0.9225\n",
        "10000/10000 [==============================] - 0s 27us/sample - loss: 0.2613 - acc: 0.9265\n",
        "loss= 0.26132496373653413\n",
        "acc= 0.9265\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZyRDXoX_Stx"
      },
      "source": [
        "# -255 ~ 255로 하면"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVcxZIVr-mJd"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# train_x = train_x/255\n",
        "# test_x = test_x/255\n",
        "train_x = train_x*2 - 255\n",
        "test_x = test_x*2 - 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT09nVdp_ZFX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "cf657248-597a-4670-fbde-99017558e389"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(train_x, train_y, epochs=5, verbose=1, batch_size=128)\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss=\",loss)\n",
        "print(\"acc=\",acc)\n",
        "\n",
        "y_ = model.predict(test_x)\n",
        "predicted = np.argmax(y_, axis=1)\n",
        "\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 8,070\n",
            "Trainable params: 8,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 4.1636 - acc: 0.1091\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 2.3027 - acc: 0.1123\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 2.3011 - acc: 0.1126\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 2.2933 - acc: 0.1176\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 2.1994 - acc: 0.1655\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 2.1274 - acc: 0.1931\n",
            "loss= 2.127422957611084\n",
            "acc= 0.1931\n",
            "[1 2 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXRsbuhi_fxp"
      },
      "source": [
        "학습이 진행되지만 normalization 안했을 때 보다 더디게 진행된다.\n",
        "\n",
        "아래는 normalization을 안했을 때\n",
        "```\n",
        "Epoch 1/5\n",
        "60000/60000 [==============================] - 1s 18us/sample - loss: 2.4654 - acc: 0.1538\n",
        "Epoch 2/5\n",
        "60000/60000 [==============================] - 1s 19us/sample - loss: 1.9504 - acc: 0.2509\n",
        "Epoch 3/5\n",
        "60000/60000 [==============================] - 1s 17us/sample - loss: 1.7553 - acc: 0.3037\n",
        "Epoch 4/5\n",
        "60000/60000 [==============================] - 1s 18us/sample - loss: 1.6404 - acc: 0.3354\n",
        "Epoch 5/5\n",
        "60000/60000 [==============================] - 1s 18us/sample - loss: 1.5597 - acc: 0.3471\n",
        "10000/10000 [==============================] - 0s 26us/sample - loss: 1.5352 - acc: 0.3417\n",
        "loss= 1.5351685056686402\n",
        "acc= 0.3417\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLZWVTr_Abcy"
      },
      "source": [
        "# 0 ~ 0.5로 하면"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xE5wv_w_ceD"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# train_x = train_x/255\n",
        "# test_x = test_x/255\n",
        "train_x = train_x/255/2\n",
        "test_x = test_x/255/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFyXibot_1Ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "ba47fca6-e316-4233-d89f-5fb5531dbbad"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(train_x, train_y, epochs=5, verbose=1, batch_size=128)\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss=\",loss)\n",
        "print(\"acc=\",acc)\n",
        "\n",
        "y_ = model.predict(test_x)\n",
        "predicted = np.argmax(y_, axis=1)\n",
        "\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 8,070\n",
            "Trainable params: 8,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.0266 - acc: 0.6823\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.4344 - acc: 0.8783\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3587 - acc: 0.8995\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3184 - acc: 0.9108\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2932 - acc: 0.9169\n",
            "10000/10000 [==============================] - 0s 31us/sample - loss: 0.2841 - acc: 0.9174\n",
            "loss= 0.28412638966143133\n",
            "acc= 0.9174\n",
            "[7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsHcOKyP_9T3"
      },
      "source": [
        "학습이 되며 0~1로 normalization했을 때 보다 살짝 더디다\n",
        "\n",
        "아래는 0~1로 normalization을 했을 때\n",
        "```\n",
        "Epoch 1/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.8862 - acc: 0.7278\n",
        "Epoch 2/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3659 - acc: 0.8950\n",
        "Epoch 3/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3164 - acc: 0.9100\n",
        "Epoch 4/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2904 - acc: 0.9171\n",
        "Epoch 5/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2724 - acc: 0.9225\n",
        "10000/10000 [==============================] - 0s 27us/sample - loss: 0.2613 - acc: 0.9265\n",
        "loss= 0.26132496373653413\n",
        "acc= 0.9265\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Wm0dNTAuEb"
      },
      "source": [
        "# 0 ~ 2로 하면"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EXcTUBy_5xN"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# train_x = train_x/255\n",
        "# test_x = test_x/255\n",
        "train_x = train_x/255*2\n",
        "test_x = test_x/255*2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cKj6SChAxPz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "c14077b6-c3d9-41b9-acb9-e07a9fa33edf"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(train_x, train_y, epochs=5, verbose=1, batch_size=128)\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss=\",loss)\n",
        "print(\"acc=\",acc)\n",
        "\n",
        "y_ = model.predict(test_x)\n",
        "predicted = np.argmax(y_, axis=1)\n",
        "\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_7 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 8,070\n",
            "Trainable params: 8,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.7630 - acc: 0.7654\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3363 - acc: 0.9045\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2965 - acc: 0.9152\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2724 - acc: 0.9212\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2548 - acc: 0.9267\n",
            "10000/10000 [==============================] - 0s 31us/sample - loss: 0.2541 - acc: 0.9268\n",
            "loss= 0.25408969558775424\n",
            "acc= 0.9268\n",
            "[7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ZadaUTA3Qd"
      },
      "source": [
        "학습이 되며 0~1로 normalization했을 때 보다 살짝 빠르다\n",
        "\n",
        "아래는 0~1로 normalization을 했을 때\n",
        "```\n",
        "Epoch 1/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.8862 - acc: 0.7278\n",
        "Epoch 2/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3659 - acc: 0.8950\n",
        "Epoch 3/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3164 - acc: 0.9100\n",
        "Epoch 4/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2904 - acc: 0.9171\n",
        "Epoch 5/5\n",
        "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2724 - acc: 0.9225\n",
        "10000/10000 [==============================] - 0s 27us/sample - loss: 0.2613 - acc: 0.9265\n",
        "loss= 0.26132496373653413\n",
        "acc= 0.9265\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtntCeHWW42I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}